<p>py2spark</p>
<pre><code class="lang-python">from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark import SparkConf, SparkContext
import time

#Rank of disciplines based on number of visits (regardless the country or the portal)

start_time = time.time()

conf = SparkConf().setMaster(&quot;local[*]&quot;)
sc = SparkContext(conf = conf)

def loadDisciplines():
    disciplineNames = {}
    with open(&quot;/Users/espoelst/Documents/JADS/DataEngineering/StudyPortals/Spark/Discipline_synonym.csv&quot;) as f:
        for line in f:
            try:
                fields = line.split(&#39;,&#39;)
                disciplineNames[float(fields[1].strip(&#39;u&#39;))] = fields[0]
            except:
                pass
    return disciplineNames

def parseLine(line):
    fields = line.split(&#39;,&#39;)
    try:
        discipline = int(fields[2].strip(&#39;&quot;&#39;))
        country = int(fields[2].strip(&#39;&quot;&#39;))
        portals = int(fields[1].strip(&#39;&quot;&#39;))
        visited_date = fields[0]
    except:
        pass
    return discipline,country,portals,visited_date  


# The windows thingy Create a SparkSession (the config bit is only for Windows!)

spark = SparkSession \
    .builder \
    .appName(&quot;Python Spark SQL basic example&quot;) \
    .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \
    .getOrCreate()

nameDict = loadDisciplines()

# Get the raw data
lines = sc.textFile(&quot;file:///Users/espoelst/Documents/JADS/DataEngineering/StudyPortals/Spark/date_url_encoded.csv&quot;)
# Convert it to a RDD of Row objects
#movies = lines.map(lambda x: Row(Discipline = parseLine(x)))

movies = lines.map(lambda x: Row(discipline = parseLine(x)[0]))
# Convert that to a DataFrame
movieDataset = spark.createDataFrame(movies)
#movieDataset = movieDataset.filter(movieDataset[&#39;country&#39;] == &#39;switzerland&#39;)
#movieDataset = movieDataset.filter(movieDataset[&#39;portals&#39;] == &#39;www.phdportal.com&#39;)
#movieDataset = movieDataset.filter(movieDataset[&#39;visitedDate&#39;] == &#39;2016-12-10&#39;)

try:
    topDisciplines = movieDataset.groupBy(&quot;Discipline&quot;).count().orderBy(&quot;count&quot;, ascending=False).cache()
    topDisciplines.show(100)
except:
    movieDataset.show(10)

top10 = topDisciplines.take(10)

#topDisciplines = hiveContext.sql(&#39;&quot;&quot;&#39;SELECT discipline,COUNT(discipline) FROM movieDataset WHERE country=switzerland and portals=www.phdportal.com AND visited_date=2016-12-10 GROUP BY discipline&#39;&quot;&quot;&#39;)
#topDisciplines.show()
#SELECT discipline,COUNT(discipline) FROM date_url_encoded WHERE country=39 and portals=2 AND visited_date=381 GROUP BY discipline

#Print the results
print(&quot;\n&quot;)
for result in top10:
    #Each row has movieID, count as above.
    try:
        print(&quot;%s: %s&quot; % (nameDict[result[0]], result[1]))
    except:
        pass

elapsed_time = time.time() - start_time
print(elapsed_time)
# Stop the session
#spark.stop()

#topDisciplines.to_csv(&quot;file:///Users/espoelst/Documents/JADS/DataEngineering/StudyPortals/Spark/Output.csv&quot;)
</code></pre>
